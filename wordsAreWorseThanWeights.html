<!DOCTYPE html>
<html lang="en">
<head>
 <meta charset="utf-8">
 <title>Words are Worse than Weights</title>
 <script>/* This HTML was generated by 📜 Scroll v108.3.1. https://scroll.pub */</script>
 <style>@media print {.doNotPrint {display: none !important;}}</style>
 <link rel="canonical" href="https://breckyunits.com/wordsAreWorseThanWeights.html">
 <meta name="viewport" content="width=device-width,initial-scale=1">
 <meta name="description" content="January 12, 2024 — For decades I had a bet that worked in good times and bad: time you invest in word skills easily pays for itself via increased value you can provide to society. If the tide went out for me I'd pick up a book on a new programming language so that when the tide came back in I'd be b">
 <meta name="generator" content="Scroll v108.3.1">
 <meta property="og:title" content="Words are Worse than Weights">
 <meta property="og:description" content="January 12, 2024 — For decades I had a bet that worked in good times and bad: time you invest in word skills easily pays for itself via increased value you can provide to society. If the tide went out for me I'd pick up a book on a new programming language so that when the tide came back in I'd be b">
 <meta property="og:image" content="https://breckyunits.com/literacy.png">
 <link rel="alternate" type="application/rss+xml" title="Words are Worse than Weights" href="feed.xml">
 <meta name="twitter:card" content="summary_large_image">
</head>
<body>
<link rel="stylesheet" type="text/css" href="gazette.css"></link>

<style>
.pageHeader {
  position: absolute;
  top: .25rem;
  right: 0;
  left: 0;
}
.pageHeader svg {
  width: 1.875rem;
  height: 1.875rem;
  fill: rgba(204,204,204,.8);
}
.pageHeader svg:hover {
  fill: #333;
}
.pageHeader a {
  color: rgba(204,204,204,.8);
  position: absolute;
  font-size: 1.875rem;
  line-height: 1.7rem;
  text-decoration: none;
}
.pageHeader a:hover {
  color: #333;
}

</style><div class="pageHeader doNotPrint">
 <a style="left:.1875rem;" href="bipolarKeto.html">&lt;</a>
 <a style="text-align:left;left:1.5625rem;" href="index.html"><svg role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12.7166 3.79541C12.2835 3.49716 11.7165 3.49716 11.2834 3.79541L4.14336 8.7121C3.81027 8.94146 3.60747 9.31108 3.59247 9.70797C3.54064 11.0799 3.4857 13.4824 3.63658 15.1877C3.7504 16.4742 4.05336 18.1747 4.29944 19.4256C4.41371 20.0066 4.91937 20.4284 5.52037 20.4284H8.84433C8.98594 20.4284 9.10074 20.3111 9.10074 20.1665V15.9754C9.10074 14.9627 9.90433 14.1417 10.8956 14.1417H13.4091C14.4004 14.1417 15.204 14.9627 15.204 15.9754V20.1665C15.204 20.3111 15.3188 20.4284 15.4604 20.4284H18.4796C19.0806 20.4284 19.5863 20.0066 19.7006 19.4256C19.9466 18.1747 20.2496 16.4742 20.3634 15.1877C20.5143 13.4824 20.4594 11.0799 20.4075 9.70797C20.3925 9.31108 20.1897 8.94146 19.8566 8.7121L12.7166 3.79541ZM10.4235 2.49217C11.3764 1.83602 12.6236 1.83602 13.5765 2.49217L20.7165 7.40886C21.4457 7.91098 21.9104 8.73651 21.9448 9.64736C21.9966 11.0178 22.0564 13.5119 21.8956 15.3292C21.7738 16.7067 21.4561 18.4786 21.2089 19.7353C20.9461 21.0711 19.7924 22.0001 18.4796 22.0001H15.4604C14.4691 22.0001 13.6655 21.1791 13.6655 20.1665V15.9754C13.6655 15.8307 13.5507 15.7134 13.4091 15.7134H10.8956C10.754 15.7134 10.6392 15.8307 10.6392 15.9754V20.1665C10.6392 21.1791 9.83561 22.0001 8.84433 22.0001H5.52037C4.20761 22.0001 3.05389 21.0711 2.79113 19.7353C2.54392 18.4786 2.22624 16.7067 2.10437 15.3292C1.94358 13.5119 2.00338 11.0178 2.05515 9.64736C2.08957 8.73652 2.55427 7.91098 3.28346 7.40886L10.4235 2.49217Z"/></svg></a>
 <a style="text-align:right;right: 1.5625rem;" title="View Source" href="https://github.com/breck7/breckyunits.com/blob/main/wordsAreWorseThanWeights.scroll"><svg xmlns="http://www.w3.org/2000/svg" width="92pt" height="92pt" viewBox="0 0 92 92"><path d="M90.156 41.965 50.036 1.848a5.913 5.913 0 0 0-8.368 0l-8.332 8.332 10.566 10.566a7.03 7.03 0 0 1 7.23 1.684 7.043 7.043 0 0 1 1.673 7.277l10.183 10.184a7.026 7.026 0 0 1 7.278 1.672 7.04 7.04 0 0 1 0 9.957 7.045 7.045 0 0 1-9.961 0 7.038 7.038 0 0 1-1.532-7.66l-9.5-9.497V59.36a7.04 7.04 0 0 1 1.86 11.29 7.04 7.04 0 0 1-9.957 0 7.04 7.04 0 0 1 0-9.958 7.034 7.034 0 0 1 2.308-1.539V33.926a7.001 7.001 0 0 1-2.308-1.535 7.049 7.049 0 0 1-1.516-7.7L29.242 14.273 1.734 41.777a5.918 5.918 0 0 0 0 8.371L41.855 90.27a5.92 5.92 0 0 0 8.368 0l39.933-39.934a5.925 5.925 0 0 0 0-8.371"/></svg></a>
 <a style="right:.1875rem;" href="ifNatureIsDoingIt.html">&gt;</a>
</div>

<style>img {border: 2px solid #e9e9e9; border-radius: 5px;}</style>
<div class="scrollSection"><h1 class="scrollTitle"><a href="wordsAreWorseThanWeights.html">Words are Worse than Weights</a></h1>
</div>
<div class="scrollColumns" style="column-width:35ch;column-count:2;max-width:90ch;">
<p class="scrollParagraph"><span class="scrollDateline">January 12, 2024 — </span>For decades I had a bet that worked in good times and bad: time you invest in word skills easily pays for itself via increased value you can provide to society. If the tide went out for me I'd pick up a book on a new programming language so that when the tide came back in I'd be better equipped to contribute more. I also thought that the more society invested in words, the better off society would be. New words and word techniques from scientific research helped us invent new technology and cure disease. Improvements in words led to better legal and commerce and diplomatic systems that led to more justice and prosperity for more people. My read on history is that it was words that led to the start of civilization, words were our present, and words were our future. <strong>Words were the safe bet</strong>.</p>
<p class="scrollParagraph">Words were the best way to model the world. I had little doubt. The computing revolution enabled us to gather and utilize more words than ever before. The path to progress seemed clear: continue to invent useful words and arrange these words in better ways to enable more humans to live their best lives. Civilization would build a collective world model out of words, encoding all new knowledge mined by science, and this would be packaged in a program everyone would have access to.</p>
<div class="scrollSection"><h2 class="scrollParagraph">...along come the neural networks of 2022-2023</h2>
</div>
<p class="scrollParagraph">I believed in word models. Then ChatGPT, Midjourney and their cousins crushed my beliefs. These programs are not powered by word models. They are powered by weight models. Huge amounts of intertwined linked nodes. Knowledge of concepts scattered across intermingled connections, not in discrete blocks. Trained, not constructed.</p>
<p class="scrollParagraph">Word models are inspectable. You plug in your inputs and can follow them through a sequence of discrete nameable steps to get to the outputs of the model. Weight models, in contrast, have huge matrices of numbers in the middle and do not need to have discrete nameable intermediate steps to get to their output. The understandability of their internal models is not so important if the model performs well enough.</p>
<p class="scrollParagraph">And these weight models are <em>amazing</em>. Their performance is undeniable.</p>
<p class="scrollParagraph">I <em>hate</em> this! I hate being wrong, but I especially hate being wrong about <em>this</em>. About words! That words are <em>not</em> the future of world models. That the future is in weight models. <strong>Weights are the safe bet</strong>. I hate being wrong that words are worse than weights. I hate being wrong about my most core career bet, that time improving my word skills would always have a good ROI.</p>
<div class="scrollSection"><h2 class="scrollParagraph">Game over for words</h2>
<p class="scrollParagraph">In the present the race seems closer but if you project trends it is game over. Not only are words worse than weights, but I see no way for words to win. The future will show words are <em>far worse</em> than weights for modeling things. We will see artificial agents in the future that will be able to predict the weather, sing, play any instrument, walk, ride bikes, drive, fly, tend plants, perform surgery, construct buildings, run wet labs, manufacture things, adjudicate disputes--do it all. They will not be powered by word models. They will be powered by weights. Massive numbers of numbers. Self-trained from massive trial and error, not taught from a perfect word model.</p>
</div>
<p class="scrollParagraph">These weight models will contain submodels to communicate with us in words, at least for a time. But humans will not be able to keep up and understand what is going on. Our word models will seem as feeble to the AIs as a pet dog's model of the world seems to its owner.</p>
<figure class="scrollCaptionedFigure"><a href="literacy.png" target="_blank"  ><img src="literacy.png" width="3400" height="2400" loading="lazy"></a><figcaption><p class="scrollParagraph">Literacy has historically had a great ROI but its value in the future is questionable as artificial agents with weight brains will perform so much better than agents operating with word brains.</p></figcaption></figure>
<p class="scrollParagraph">Things we value today, like knowing the periodic table, or the names of capital cities, or biological pathways--word models to understand our world--will be irrelevant. The digital weight models will handle things with their own understanding of the world which will leave ours further and further in the dust. We are now in the early days where these models are still learning their weights from our words, but it won't be long before these agents "take it from here" and begin to learn everything on their own from scratch, and come up with arrangements of weights that far outperform our word based world models. Sure, the hybrid era where weight models work alongside humans with their word models will last for a time, but at some point the latter will become inconsequential agents in this world.</p>
<div class="scrollSection"><h2 class="scrollParagraph">Weights run the world</h2>
<p class="dropcap">Now I wonder if I always saw the world wrong. I see how words will be less valuable in the future. But now I also see that I likely greatly overvalued words in our present. Words not synchronized to brains are inert. To be useful, words require weights, but weights don't require words. Words are guidelines. Weights are the substance. Everything is run by weights, not words. Words are correlated with reality, but it is weights that really make the decisions. Word mottos don't run humans, as much as we try. Words correlate, but it is our neural weights that run things. Words are not running the economy. Weights are and always have been. The economy is in a sense the first blackbox weight powered artificial intelligence. Word models correlate with reality but are very leaky models. There are far more "unwritten rules" than written rules.</p>
</div>
<p class="scrollParagraph">I have long devalued narratives but highly valued words in the form of datasets. But datasets are also far less valuable than weights. I used to say "the pen is mightier than the sword, but the CSV is mightier than the pen." Now I see that weights are far mightier than the CSV!</p>
<p class="scrollParagraph">Words are worse not just because of our current implementations. Fundamentally word models discretize a universe into discrete concepts that do not exist. The real world is fuzzier and more continuous. Weights don't have to discretize things. They just need to perform. Now that we have hardware to run weight models of sufficient size, it is clear that word models are fundamentally worse. As hardware and techniques improve, the gap will grow. Weights interpolate better. As artificial neural networks are augmented with embodiment and processes resembling consciousness, they will be able to independently expand the frontiers of their training data.</p>
<p class="scrollParagraph">Nature does not owe us a word model of the universe. Just because part of my brain desperately <em>wants</em> an understanding of the world in words it is not like there was a deal in place. If truth means an accurate prediction of the past, present, and future, weight models serve that better than word models. I can close my eyes to it all I want but when I look at the data I see weights work better.</p>
<div class="scrollSection"><h2 class="scrollParagraph">Overcorrecting?</h2>
</div>
<p class="dropcap">Could I be wrong again? I was once so biased in favor of words. In 2019 I gave a lightning talk at a program synthesis conference alongside early researchers from OpenAI. I claimed that neural nets were still far from fluency and to get better computational agents we needed to find novel simpler word systems designed for humans and computers. But then OpenAI has shown that LLMs have no trouble mastering even the most complex of human languages. The potential of weights was right in front of me but I stubbornly kept betting on words. So my track record in predicting the future on this topic isn't exactly stellar. Maybe me switching my bet away from words now is actually a sign that it is time to bet on words again!</p>
<p class="scrollParagraph">But I don't think so. I was probably 55-45 back then, in favor of words. I think in large part I bet on words because so many people in the program synthesis world were betting on weights, so I saw taking the contrarian bet as the one with the higher expected value for me. <strong>Now I am 500 to 1 that weights are the future</strong>.</p>
<p class="scrollParagraph">The long time I spent betting on words makes me more confident that words are doomed. For years I tried thousands and thousands of paths to find some way to make word models radically better. I've also searched the world for people smarter than me who were trying to do that. <a href="https://en.wikipedia.org/wiki/Cyc">Cyc</a> is one of the more famous attempts that came up short. It is not that they failed to write all unwritten rules it is that nature's rules are likely unwriteable. <a href="https://www.wolfram.com/mathematica/">Wolfram Mathematica</a> has made far more progress and is a very useful tool, but it seems clear that its word system will never achieve the takeoff that a learning weights based system will. Again, the race at the moment seems close, but weights have started to pull away. If there was a path for word models to win I think I would have glimpsed it by now.</p>
<p class="scrollParagraph">The only thing I can think of is that there actually will turn out to be some algebra of compression that would make the best performing weight models isomorphic to highly refined word models. But that seems far more like wishful thinking from some biased neural agents in my brain that formed for word models and want to justify their existence.</p>
<p class="scrollParagraph">It seems much more probable that nature favors weight models, and that we are near or may have even passed peak word era. Words were nature's tool to generate knowledge faster than genetic evolution in a way that could be transferred across time and space, but at the cost of speed and prediction accuracy, and now we evolved a way where knowledge can be transferred across time and space and have much better speed and prediction accuracy than words.</p>
<p class="scrollParagraph">Words will go the way of Latin. Words will become mostly a relic. Weights are the future. Words are not dead yet. But words are dead.</p>
<div class="scrollSection"><h2 class="scrollParagraph">Looking ahead</h2>
</div>
<p class="dropcap">I will always enjoy playing with words as a hobby. Writing essays like these, where I try to create a word model for some aspect of the world, makes me feel better, when I reach some level of satisfaction with the model I wrestle with. But how useful will skills with words be for society? Is it still worth honing my programming skills? For the first time in my life it seems like the answer is no. I guess it was a blessing to have that safe bet for so long. Pretty sad to see it go. But I don't see how words will put food on the table. If you need me I'll be out scrambling to find the best way to bet on weights.</p>
<div class="dinkus"><span>⁂</span></div>
<div class="scrollSection"><h1 class="scrollParagraph">Related Reading</h1>
<ul ><li ><a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">The Bitter Lesson (2019)</a></li>
<li ><a href="https://moalquraishi.wordpress.com/2018/12/09/alphafold-casp13-what-just-happened/">What Just Happened? (2018)</a></li></ul>
</div>
<div class="scrollKeyboardNav" style="display:none;"><a href="bipolarKeto.html">bipolarKeto.html</a> · wordsAreWorseThanWeights.html · <a href="ifNatureIsDoingIt.html">ifNatureIsDoingIt.html</a><script>document.addEventListener('keydown', function(event) {
  if (document.activeElement !== document.body) return
  const getLinks = () => document.getElementsByClassName("scrollKeyboardNav")[0].getElementsByTagName("a")
  if (event.key === "ArrowLeft")
    getLinks()[0].click()
  else if (event.key === "ArrowRight")
    getLinks()[1].click()
 });</script></div>
</div><style>.pageFooter {
  margin-top: 8px;
  padding-top: 8px;
  text-align: center;
}
.pageFooter svg {
  width: 30px;
  height: 30px;
  fill: rgba(204,204,204, .5);
  padding: 0 7px;
}
.pageFooter svg:hover {
  fill: #333;
}
.pageFooterScrollLink {
  font-family: Verdana;
  font-weight: 100;
  margin: .5em;
}
.pageFooterScrollLink a {
  color: rgba(204,204,204,.5);
}
.pageFooterScrollLink a:hover {
  color: #333;
  text-decoration: none;
}
</style><p class="scrollViewSource doNotPrint"></p>
<div class="pageFooter doNotPrint">
 <a href="mailto:breck7@gmail.com"><svg viewBox="3 5 24 20" width="24" height="20" xmlns="http://www.w3.org/2000/svg"><g transform="matrix(1, 0, 0, 1, 0, -289.0625)"><path style="opacity:1;stroke:none;stroke-width:0.49999997;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1" d="M 5 5 C 4.2955948 5 3.6803238 5.3628126 3.3242188 5.9101562 L 14.292969 16.878906 C 14.696939 17.282876 15.303061 17.282876 15.707031 16.878906 L 26.675781 5.9101562 C 26.319676 5.3628126 25.704405 5 25 5 L 5 5 z M 3 8.4140625 L 3 23 C 3 24.108 3.892 25 5 25 L 25 25 C 26.108 25 27 24.108 27 23 L 27 8.4140625 L 17.121094 18.292969 C 15.958108 19.455959 14.041892 19.455959 12.878906 18.292969 L 3 8.4140625 z " transform="translate(0,289.0625)" id="rect4592"/></g></svg></a>
 <a href="download.html"><svg fill="#000000" xmlns="http://www.w3.org/2000/svg" width="800px" height="800px" viewBox="0 0 52 52" enable-background="new 0 0 52 52" xml:space="preserve"><path d="M38.6,20.4c-1-6.5-6.7-11.5-13.5-11.5c-7.6,0-13.7,6.1-13.7,13.7c0,0.3,0,0.7,0.1,1c-5,0.4-8.9,4.6-8.9,9.6 c0,5.4,4.3,9.7,9.7,9.7h11.5c-0.8-0.8-8.1-8.1-8.1-8.1c-0.4-0.4-0.4-0.9,0-1.3l1.3-1.3c0.4-0.4,0.9-0.4,1.3,0l3.5,3.5 c0.4,0.4,1.1,0.1,1.1-0.4V21.8c0-0.4,0.5-0.9,1-0.9h1.9c0.5,0,0.9,0.4,0.9,0.9v13.4c0,0.6,0.8,0.8,1.1,0.4l3.5-3.5 c0.4-0.4,0.9-0.4,1.3,0l1.3,1.3c0.4,0.4,0.4,0.9,0,1.3L26,42.9h12.3v0c6.1-0.1,11-5.1,11-11.3C49.4,25.5,44.6,20.6,38.6,20.4z"/></svg></a>
 <a title="View Source" href="https://github.com/breck7/breckyunits.com/blob/main/wordsAreWorseThanWeights.scroll"><svg xmlns="http://www.w3.org/2000/svg" width="92pt" height="92pt" viewBox="0 0 92 92"><path d="M90.156 41.965 50.036 1.848a5.913 5.913 0 0 0-8.368 0l-8.332 8.332 10.566 10.566a7.03 7.03 0 0 1 7.23 1.684 7.043 7.043 0 0 1 1.673 7.277l10.183 10.184a7.026 7.026 0 0 1 7.278 1.672 7.04 7.04 0 0 1 0 9.957 7.045 7.045 0 0 1-9.961 0 7.038 7.038 0 0 1-1.532-7.66l-9.5-9.497V59.36a7.04 7.04 0 0 1 1.86 11.29 7.04 7.04 0 0 1-9.957 0 7.04 7.04 0 0 1 0-9.958 7.034 7.034 0 0 1 2.308-1.539V33.926a7.001 7.001 0 0 1-2.308-1.535 7.049 7.049 0 0 1-1.516-7.7L29.242 14.273 1.734 41.777a5.918 5.918 0 0 0 0 8.371L41.855 90.27a5.92 5.92 0 0 0 8.368 0l39.933-39.934a5.925 5.925 0 0 0 0-8.371"/></svg></a>
 <div class="pageFooterScrollLink">
  <a href="https://scroll.pub">Built with Scroll v108.3.1</a>
</div>
</div>
</body>
</html>